# -*- coding: utf-8 -*-
"""SBFXH.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sMd9RTtFqGAaTzvD70G6n2wCHkZT33eC
"""

from __future__ import print_function
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.optim.lr_scheduler import StepLR
import matplotlib.pyplot as plt
import numpy as np

class DatasetSplit(torch.utils.data.Dataset):
    def __init__(self, dataset, idxs):
        self.dataset = dataset
        self.idxs = list(idxs)

    def __len__(self):
        return len(self.idxs)

    def __getitem__(self, item):
        image, label = self.dataset[self.idxs[item]]
        return image, label

def cifar_noniid(dataset, num_users):
    """
    Sample non-I.I.D client data from CIFAR10 dataset
    :param dataset:
    :param num_users:
    :return:
    """
    num_aloc = 5
    num_shards, num_imgs = num_users * num_aloc, int(len(dataset) / (num_users * num_aloc))
    idx_shard = [i for i in range(num_shards)]
    dict_users = {i: np.array([], dtype='int64') for i in range(num_users)}
    idxs = np.arange(num_shards * num_imgs)
    # labels = dataset.train_labels.numpy()
    labels = np.array(dataset.targets)
    # sort labels
    idxs_labels = np.vstack((idxs, labels))
    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]
    idxs = idxs_labels[0, :]
    # divide and assign
    for i in range(num_users):
        rand_set = set(np.random.choice(idx_shard, num_aloc, replace=False))
        idx_shard = list(set(idx_shard) - rand_set)
        for rand in rand_set:
            dict_users[i] = np.concatenate((dict_users[i], idxs[rand * num_imgs:(rand + 1) * num_imgs]), axis=0)
        dict_users[i] = DatasetSplit(dataset, dict_users[i])
    return dict_users

def showdata(train_loader):
  fig = plt.figure()
  for batch_idx, (data, target) in enumerate(train_loader):
    if batch_idx == 6:
      break
    print(batch_idx)
    print(data.shape)
    print(target.shape)
    plt.subplot(2, 3, batch_idx + 1)
    plt.tight_layout()
    data= data.reshape(-1,28)
    plt.imshow(data, cmap='gray', interpolation='none')
    plt.title("Ground Truth: {}".format(target))
    plt.xticks([])
    plt.yticks([])
    plt.show()



# transform=transforms.Compose([
# transforms.ToTensor()
#  ,transforms.Normalize((0.1307,), (0.3081,))
# ])
transform = transforms.Compose([
  transforms.ToTensor(),
  transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
])
# dataset1 = datasets.MNIST('../data', train=True, download=True, transform=transform)
# dataset2 = datasets.MNIST('../data', train=False, transform=transform)
dataset1 = datasets.CIFAR10('../data', train=True, download=True, transform=transform)
dataset2 = datasets.CIFAR10('../data', train=False, download=True, transform=transform)

# dataset1 =

# train_loader = torch.utils.data.DataLoader(dataset1)
test_loader = torch.utils.data.DataLoader(dataset2, batch_size=1024)

import random
import math
m = 50
d = []
X = []
Y = []
for i in range(m+1):
    d.append([])

for i in range(m):
    X.append(random.random()*400)
    Y.append(random.random()*400)

X.append(200)
Y.append(200)

for i in range(m+1):
    for j in range(m+1):
      d[i].append(((X[i]-X[j])**2+(Y[i]-Y[j])**2)**0.5)

dataset1 = cifar_noniid(dataset1, m)

bound = 80

from ast import Continue

accuracy = []
Loss = []
FedSGD_TAR_Loss = []
FedSGD_STAR_Loss = []
FedSGD_TARRM_Loss = []
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.optim.lr_scheduler import StepLR
import matplotlib.pyplot as plt
import numpy as np
from torchvision.models import resnet18

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        # self.fc1 = nn.Linear(9216, 128)
        self.fc1 = nn.Linear()
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        output = F.log_softmax(x, dim=1)
        return output

class CNNCifar(nn.Module):
    def __init__(self, num_classes):
        super(CNNCifar, self).__init__()
        # self.conv1 = nn.Conv2d(3, 6, 5)
        # self.pool = nn.MaxPool2d(2, 2)
        # self.conv2 = nn.Conv2d(6, 16, 5)
        self.conv1 = nn.Conv2d(3, 64, 5)
        self.pool = nn.MaxPool2d(3, 2)
        self.conv2 = nn.Conv2d(64, 64, 5)
        # self.fc1 = nn.Linear(16 * 5 * 5, 120)
        # self.fc2 = nn.Linear(120, 84)
        # self.fc3 = nn.Linear(84, args.num_classes)
        self.fc1 = nn.Linear(64 * 4 * 4, 768)
        self.fc2 = nn.Linear(768, 384)
        self.fc3 = nn.Linear(384, num_classes)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        #x = x.view(-1, 16 * 5 * 5)
        x = x.view(-1, 64 * 4 * 4)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return F.log_softmax(x, dim=1)

class Wrapper(nn.Module):
  def __init__(self, num_classes):
    super(Wrapper, self).__init__()
    self.model = resnet18(weights=None, num_classes=10)
    self.model.conv1 = nn.Conv2d(3, 64, kernel_size=3,
              stride=1, padding=1, bias=False)

  def forward(self, x):
    x = self.model.conv1(x)
    x = self.model.bn1(x)
    x = self.model.relu(x)
    # x = self.maxpool(x)

    x = self.model.layer1(x)
    x = self.model.layer2(x)
    x = self.model.layer3(x)
    x = self.model.layer4(x)

    x = self.model.avgpool(x)
    x = torch.flatten(x, 1)
    x = self.model.fc(x)
    return F.log_softmax(x, dim=1)

def aggregate(target, source, k):
    for target_param, param in zip(target.parameters(), source.parameters()):
        target_param.data.copy_(target_param.data + param.data * 1.0/k)


def MST(vis, G):
    vis[0] = 1
    T = 0
    for n in range(m):
        minT = 2023
        fr = 0
        to = 0
        for i in range(m):
            for j in range(m):
                if(i == j):
                    continue
                x = np.random.normal(0, 1)
                y = np.random.normal(0, 1)
                z = (x**2 + y**2)**0.5
                t = 8.443/(100*np.log2(1+z*0.1/((d[i][j])**4)/1e-11))
                if(vis[i] == 1 and vis[j] == 0):
                    if (t < minT):
                        fr = i
                        to = j
                        minT = t
        vis[to] = 1
        G[fr, to] = 1
        if(minT != 2023):
            T += minT
    return (G, T)


def MST_aggregate(x, vis, G, targets, models):
    vis[x] = 1
    for target_param, param in zip(targets[x].parameters(), models[x].parameters()):
        target_param.data.copy_(target_param + param)
    for i in range(m):
        if(vis[i] > 0 or np.random.rand() < 0.0):
            continue
        if(G[x, i] > 0):
            MST_aggregate(i, vis, G, targets, models)
            for target_param, param in zip(targets[x].parameters(), targets[i].parameters()):
                target_param.data.copy_(target_param + param)



def train(models, dataset, optimizers, epoch):
    sum_loss = 0
    for key, value in dataset.items():
        # print(value)
        train_loader = torch.utils.data.DataLoader(value, batch_size=64, shuffle=True)

        idx = np.arange(len(train_loader))
        idx = np.random.choice(idx, 4, replace=True)
        for batch_idx, (data, target) in enumerate(train_loader):
            if batch_idx in idx:
                data = data.cuda()
                target = target.cuda()
                output = models[key](data)
                loss = F.nll_loss(output, target)
                optimizers[key].zero_grad()
                loss.backward()
                optimizers[key].step()

                #print('Train Epoch: {} {}th clinet\tLoss: {:.6f}'.format(epoch, key, loss.item()))
                sum_loss += loss.item()
        lrs[key].step()
    #print(sum_loss)




def test(model, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for batch_idx, (data, target) in enumerate(test_loader):
            data = data.cuda()
            target = target.cuda()
            output = model(data)
            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss
            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability
            correct += pred.eq(target.view_as(pred)).sum().item()

            test_loss /= len(test_loader.dataset)
    # loss = 0
    # with torch.no_grad():
    #     for batch_idx, (data, target) in enumerate(train_loader):
    #         if(batch_idx == 400):
    #           break
    #         data = data.cuda()
    #         target = target.cuda()
    #         output = models[i](data)
    #         loss += F.nll_loss(output, target).item()

    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
    test_loss, correct, len(test_loader.dataset),
    100. * correct / len(test_loader.dataset)))
    accuracy.append(100. * correct / len(test_loader.dataset))
    # Loss.append(loss/m)
    # print(loss/m)
from torch.nn import init
def weights_init_xavier(m):
    classname = m.__class__.__name__
    #print(classname)
    if classname.find('Conv') != -1:
        init.xavier_normal_(m.weight.data, gain=1)
    elif classname.find('Linear') != -1:
        init.xavier_normal_(m.weight.data, gain=1)
    elif classname.find('BatchNorm') != -1:
        init.normal_(m.weight.data, 1.0, 0.02)
        init.constant_(m.bias.data, 0.0)

# transform=transforms.Compose([ transforms.ToTensor() ,transforms.Normalize((0.1307,), (0.3081,))])
# dataset1 = datasets.MNIST('../data', train=True, download=True, transform=transform)
# dataset2 = datasets.MNIST('../data', train=False, transform=transform)

# train_loader = torch.utils.data.DataLoader(dataset1,batch_size=32)
# test_loader = torch.utils.data.DataLoader(dataset2,batch_size=1)

# f = []
# cnt = []
# choose = []
# cur = []
# def find(x):
#   if(x == f[x]):
#     return x
#   else:
#     f[x] = find(f[x])
#     return f[x]

# for i in range(m):
#   f.append(i)
#   cnt.append(0)
#   choose.append(0)
#   cur.append(0)

# for i in range(m):
#   for j in range(m):
#     if d[i][j] < bound:
#       f[find(i)] = find(j)

models = []
optimizers = []
lrs = []

for i in range(m):
    # model = Net().cuda()
    model = CNNCifar(10).cuda()
    #model = Wrapper(10).cuda()
    models.append(model)
    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
    exp_lr = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=1)
    optimizers.append(opt)
    lrs.append(exp_lr)


targets = []
for i in range(m):
    targets.append(CNNCifar(10).cuda())

if __name__ == '__main__':
    for epoch in range(1, 401):
        # train(models, train_loader, optimizers, epoch)


        global_model = CNNCifar(10).cuda()
        #global_model = Wrapper(10).cuda()

        global_model.apply(weights_init_xavier)

        k = 0
        t = 0
        drop = np.zeros(m)
        if epoch!=1:
            for global_param in global_model.parameters():
                global_param.data.copy_(torch.tensor([0]))
            for i in range(m):
                if(np.random.rand() < 0.05):
                    drop[i] = 1
            for i in range(m):
                if(drop[i] == 1):
                    continue
                for global_param, param in zip(global_model.parameters(), models[i].parameters()):
                    global_param.data.copy_(global_param + param * 1.0/(m-np.sum(drop)))
            for i in range(m):
                for param, global_param in zip(models[i].parameters(), global_model.parameters()):
                      param.data.copy_(global_param)
            # t += 8.443/(100*np.log2(1+z*0.1/(d[i][m])**4/1e-11))

        if epoch == 1:
            for i in range(m):
                for param, global_param in zip(models[i].parameters(), global_model.parameters()):
                    param.data.copy_(global_param)
        # print(t)




        # for i in range(m):
        #     for target_param in targets[i].parameters():
        #         target_param.data.copy_(torch.tensor([0]))
        #     if(np.random.rand() < 0.1):
        #         aggregate(targets[i], models[i], 1)
        #         continue
        #     k = 0
        #     maxt = 0
        #     for j in range(m):
        #         if d[i][j] < bound:
        #             k += 1
        #             x = np.random.normal(0, 1)
        #             y = np.random.normal(0, 1)
        #             z = (x**2 + y**2)**0.5
        #             if i != j:
        #                 maxt = max(maxt, 8.443/(100*np.log2(1+z*0.1/(d[i][j])**4/1e-11)))
        #     t += maxt
        #     for j in range(m):
        #         if d[i][j] < bound:
        #             aggregate(targets[i], models[j], k)

        # for i in range(m):
        #     for param, target_param in zip(models[i].parameters(), targets[i].parameters()):
        #         param.data.copy_(target_param)

        # print(t)



        train(models, dataset1, optimizers, epoch)

        # for i in range(m):
        #     for target_param in targets[i].parameters():
        #         target_param.data.copy_(torch.tensor([0]))
        # vis = np.zeros(m)
        # G = np.zeros((m, m))
        # G, t = MST(vis, G)
        # # print(t)
        # vis = np.zeros(m)
        # MST_aggregate(0, vis, G, targets, models)
        # divide = np.sum(vis)
        # for i in range(m):
        #     # if(vis[i] == 0):
        #     #     continue
        #     for param, target_param in zip(models[i].parameters(), targets[0].parameters()):
        #         param.data.copy_(target_param/divide)

        # # print(t)
        if(epoch%5 == 1):
            for global_param in global_model.parameters():
                global_param.data.copy_(torch.tensor([0]))
            for i in range(m):
                for global_param, param in zip(global_model.parameters(), models[i].parameters()):
                    global_param.data.copy_(global_param + param * 1.0/m)
            test(global_model, test_loader)


        speed = np.random.normal(0, 5, (m))
        for i in range(m):
            X[i] += (2*np.random.rand()-1)*speed[i]*t
            Y[i] += (2*np.random.rand()-1)*speed[i]*t
            X[i] = max(0, X[i])
            X[i] = min(400, X[i])
            Y[i] = max(0, Y[i])
            Y[i] = min(400, Y[i])


    print(len(accuracy))
    print(accuracy)


